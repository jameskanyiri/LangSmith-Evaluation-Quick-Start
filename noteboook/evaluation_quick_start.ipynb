{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9822cf",
   "metadata": {},
   "source": [
    "# Create a dataset\n",
    " - Define example input and reference output pairs that you will use to evaluate your app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4dca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['639f8b98-6e63-4b92-8ffa-0b733494892b',\n",
       "  '7fb45806-be7a-4530-8065-aeabf2b02494',\n",
       "  'f279b217-73f8-472a-aeff-39e91bc6f442'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "#Programmatically create a dataset in langsmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"Evaluation Quick Start\",\n",
    "    description=\"A dataset for evaluating a simple language model\",\n",
    "    \n",
    ")\n",
    "\n",
    "#Create examples\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital of Kenya?\"},\n",
    "        \"outputs\": {\"answer\": \"The capital of Kenya is Nairobi\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the largest planet in the solar system?\"},\n",
    "        \"outputs\": {\"answer\": \"The largest planet in the solar system is Jupiter\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the chemical symbol for water?\"},\n",
    "        \"outputs\": {\"answer\": \"The chemical symbol for water is H2O\"},\n",
    "    },\n",
    "]\n",
    "    \n",
    "    \n",
    "client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf349712",
   "metadata": {},
   "source": [
    "# Define what your target function\n",
    "- Define a target function that defines what you are evaluating.\n",
    "- Examples of what you can test:\n",
    "    - You can test an llm call that includes the new prompt you want to evaluate.\n",
    "    - You can evaluate a part of your application\n",
    "    - You can test your end to end application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96dcf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers\n",
    "from openai import OpenAI\n",
    "\n",
    "#wrap the openai client for langsmith tracing\n",
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "\n",
    "#Define the application logic you want to evaluate inside the target function\n",
    "#The sdk will automatically send the inputs from the dataset to the target function\n",
    "\n",
    "def target(input: dict) -> dict:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer the following question accurately.\"},\n",
    "            {\"role\": \"user\", \"content\": input[\"question\"]}\n",
    "        ]\n",
    "    )\n",
    "    return {\"answer\": response.choices[0].message.content.strip()}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50621ccf",
   "metadata": {},
   "source": [
    "# Define Evaluator\n",
    "\n",
    "- Import a prebuilt prompt from openevals and create an evaluator\n",
    "    - outputs are the result of the target function\n",
    "    - reference_outputs are from the examples pair defined in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bbbfb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "def correctness_evaluator(inputs:dict, outputs:dict, reference_outputs:dict):\n",
    "    evaluator = create_llm_as_judge(\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        feedback_key=\"correctness\",\n",
    "    )\n",
    "    \n",
    "    eval_result = evaluator(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs,\n",
    "    )\n",
    "    \n",
    "    return eval_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf203c",
   "metadata": {},
   "source": [
    "# Run and view results\n",
    "\n",
    "- Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d369840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'First-eval-in-langsmith-e40bf4d7' at:\n",
      "https://smith.langchain.com/o/5e26199c-44b7-5d71-a174-0781dc496380/datasets/a7935129-510d-410a-8992-795a000b93f5/compare?selectedSessions=d548a87b-0c4a-4863-9608-f7bcc8606112\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:04,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"Evaluation Quick Start\",\n",
    "    evaluators=[correctness_evaluator],\n",
    "    experiment_prefix=\"First-eval-in-langsmith\",\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1cc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
